import os
import traceback
import pandas as pd
import subprocess
import requests
from io import StringIO
from CustomException import RequestError

table_name = lambda param: '_'.join(['t', param[0], param[1], param[2]])

logger = open(os.path.join('log', "parsing.txt"), "a")


# [è³‡ç”¢è² å‚µè¡¨, æç›Šè¡¨, ç¾é‡‘æµé‡è¡¨]
# map column names to standard codes
Standard_Code_mapping = [
{
    '1170': '1170', 
    'æ‡‰æ”¶å¸³æ¬¾æ·¨é¡': '1170', 
    '13000': '1170', # é‡‘èå…¬å¸
    'æ‡‰æ”¶æ¬¾é …ï¼æ·¨é¡': '1170', # é‡‘èå…¬å¸
    # 'æ‡‰æ”¶æ¬¾é …-æ·¨é¡': '1170', # é‡‘èå…¬å¸ # before 2013

    # ç•¶ä½œæ‡‰æ”¶å¸³æ¬¾ (éå¸¸æ…‹)
    # 'æ‡‰æ”¶ç¥¨æ“šæ·¨é¡': '1170',
    # '1150': '1170', 

    '11XX': '11XX', 
    'æµå‹•è³‡ç”¢åˆè¨ˆ': '11XX', 
    'æµå‹•è³‡ç”¢': '11XX',

    '130X': '130X',
    'å­˜è²¨': '130X',
    'å­˜è²¨åˆè¨ˆ': '130X',

    '15XX': '15XX',
    'éæµå‹•è³‡ç”¢åˆè¨ˆ': '15XX',

    '1600': '1600',
    'ä¸å‹•ç”¢ã€å» æˆ¿åŠè¨­å‚™': '1600',
    'ä¸å‹•ç”¢ã€å» æˆ¿åŠè¨­å‚™åˆè¨ˆ': '1600',
    # 'å›ºå®šè³‡ç”¢æˆæœ¬åˆè¨ˆ': '1600',  # before 2013
    # 'å›ºå®šè³‡ç”¢æ·¨é¡': '1600',  # before 2013

    '1780': '1780', 
    'ç„¡å½¢è³‡ç”¢': '1780', 
    'ç„¡å½¢è³‡ç”¢åˆè¨ˆ': '1780',
    'ç„¡å½¢è³‡ç”¢-æ·¨é¡': '1780', # é‡‘èå…¬å¸
    'ç„¡å½¢è³‡ç”¢ï¼æ·¨é¡': '1780', # é‡‘èå…¬å¸
    '19000': '1780', # é‡‘èå…¬å¸ 

    '1900': '1900',
    'å…¶ä»–éæµå‹•è³‡ç”¢': '1900',
    'å…¶ä»–éæµå‹•è³‡ç”¢åˆè¨ˆ': '1900',
    # 'å…¶ä»–è³‡ç”¢åˆè¨ˆ': '1900', # before 2013

    '1XXX': '1XXX', # ä¸€èˆ¬å…¬å¸
    'è³‡ç”¢ç¸½è¨ˆ': '1XXX', 
    'è³‡ç”¢ç¸½é¡': '1XXX',
    '19999': '1XXX',  # é‡‘èå…¬å¸
    '10000': '1XXX',  # é‡‘èå…¬å¸, (2801.TW)

    '21XX': '21XX', 
    'æµå‹•è² å‚µåˆè¨ˆ': '21XX', 
    'æµå‹•è² å‚µ': '21XX',

    '25XX': '25XX', 
    'éæµå‹•è² å‚µåˆè¨ˆ': '25XX', 

    '2600': '2600',
    'å…¶ä»–éæµå‹•è² å‚µ': '2600',
    'å…¶ä»–éæµå‹•è² å‚µåˆè¨ˆ': '2600', 
    # 'å…¶ä»–è² å‚µåˆè¨ˆ': '2600',  # before 2013
    
    # Note: å°‘æ•¸å…¬å¸æœ‰çš„è³‡è¨Š
    '3120': '3120',
    'ç‰¹åˆ¥è‚¡è‚¡æœ¬': '3120',
    
    '3100': '3100', # ä¸€èˆ¬å…¬å¸
    'è‚¡æœ¬åˆè¨ˆ': '3100', # ä¸€èˆ¬å…¬å¸ é‡‘èå…¬å¸
    '31100': '3100', # é‡‘èå…¬å¸

    '3XXX': '3XXX', # ä¸€èˆ¬å…¬å¸
    'æ¬Šç›Šç¸½è¨ˆ': '3XXX', # ä¸€èˆ¬å…¬å¸ é‡‘èå…¬å¸
    'æ¬Šç›Šç¸½é¡': '3XXX', # ä¸€èˆ¬å…¬å¸ é‡‘èå…¬å¸
    '39999': '3XXX', # é‡‘èå…¬å¸
    '30000': '3XXX', # é‡‘èå…¬å¸ (2801.TW)
    'è‚¡æ±æ¬Šç›Šç¸½è¨ˆ': '3XXX'
},

{
    '4000': '4000', 
    'ç‡Ÿæ¥­æ”¶å…¥åˆè¨ˆ': '4000', 

    '5000': '5000',
    'ç‡Ÿæ¥­æˆæœ¬åˆè¨ˆ': '5000',

    '5900': '5900', 
    'ç‡Ÿæ¥­æ¯›åˆ©ï¼ˆæ¯›æï¼‰': '5900', 
    # 'ç‡Ÿæ¥­æ¯›åˆ©(æ¯›æ)': '5900', # before 2013

    '6900': '6900', 
    'ç‡Ÿæ¥­åˆ©ç›Šï¼ˆæå¤±ï¼‰': '6900', 
    # 'ç‡Ÿæ¥­æ·¨åˆ©(æ·¨æ)': '6900', # before 2013

    # ç¨…å‰æ·¨åˆ©
    # Note: åŒæ™‚å‡ºç¾åœ¨æç›Šè¡¨ã€ç¾é‡‘æµé‡è¡¨
    '7900': '7900', # ä¸€èˆ¬å…¬å¸ é‡‘èå…¬å¸  
    'ç¹¼çºŒç‡Ÿæ¥­å–®ä½ç¨…å‰æ·¨åˆ©ï¼ˆæ·¨æï¼‰': '7900', # ä¸€èˆ¬å…¬å¸ é‡‘èå…¬å¸ # å‡ºç¾åœ¨æç›Šè¡¨ã€ç¾é‡‘æµé‡è¡¨ 
    'ç¹¼çºŒç‡Ÿæ¥­å–®ä½ç¨…å‰æç›Š': '7900', # ä¸€èˆ¬å…¬å¸ é‡‘èå…¬å¸ # å‡ºç¾åœ¨æç›Šè¡¨ã€ç¾é‡‘æµé‡è¡¨ 
    '64001': '7900', # é‡‘èå…¬å¸ (2801.TW)
    # 'ç¹¼çºŒç‡Ÿæ¥­å–®ä½ç¨…å‰åˆä½µæ·¨åˆ©ï¼ˆæ·¨æï¼‰': '7900', # ä¸€èˆ¬å…¬å¸ é‡‘èå…¬å¸ # å‡ºç¾åœ¨æç›Šè¡¨ã€ç¾é‡‘æµé‡è¡¨ # before 2013
    # 'ç¹¼çºŒç‡Ÿæ¥­å–®ä½ç¨…å‰æ·¨åˆ©(æ·¨æ)': '7900', # ä¸€èˆ¬å…¬å¸ é‡‘èå…¬å¸ # å‡ºç¾åœ¨æç›Šè¡¨ã€ç¾é‡‘æµé‡è¡¨ # before 2013

    # ç¨…å¾Œæ·¨åˆ©
    '8200': '8200' , # ä¸€èˆ¬å…¬å¸ é‡‘èå…¬å¸
    'æœ¬æœŸæ·¨åˆ©ï¼ˆæ·¨æï¼‰': '8200',
    '69000': '8200', # é‡‘èå…¬å¸
    '64000': '8200', # é‡‘èå…¬å¸ (2801.TW)
    'æœ¬æœŸç¨…å¾Œæ·¨åˆ©ï¼ˆæ·¨æï¼‰': '8200'
    # 'ç¹¼çºŒç‡Ÿæ¥­å–®ä½ç¨…å¾Œåˆä½µæ·¨åˆ©(æ·¨æ)': '8200', # before 2013
    # 'ç¹¼çºŒç‡Ÿæ¥­å–®ä½æ·¨åˆ©(æ·¨æ)': '8200', # before 2013

},

{   # é‡‘èå…¬å¸çš„ç¾é‡‘æµé‡è¡¨ï¼Œä»£è™Ÿèˆ‡åç¨±èˆ‡ä¸€èˆ¬å…¬å¸ç›¸åŒ

    # é‡‘èå…¬å¸ ä»£è™Ÿ 61001 å°æ‡‰çš„æ¬„ä½ä¸ä¸€è‡´ï¼Œæ”¹ç”¨A0010
    'A00010': '7900', # é‡‘èå…¬å¸

    # 'æŠ˜èˆŠè²»ç”¨': 'A20100', 
    # 'æ”¤éŠ·è²»ç”¨': 'A20200', Ë‹
    'AAAA': 'AAAA',   # ä¸€èˆ¬å…¬å¸ é‡‘èå…¬å¸
    'ç‡Ÿæ¥­æ´»å‹•ä¹‹æ·¨ç¾é‡‘æµå…¥ï¼ˆæµå‡ºï¼‰': 'AAAA',
    # 'ç‡Ÿæ¥­æ´»å‹•ä¹‹æ·¨ç¾é‡‘æµå…¥(æµå‡º)': 'AAAA', # before 2013
    # 'ç‡Ÿæ¥­æ´»å‹•ä¹‹æ·¨ç¾é‡‘æµå…¥': 'AAAA', # before 2013
    
    'B02700': 'B02700', # ä¸€èˆ¬å…¬å¸ é‡‘èå…¬å¸
    'å–å¾—ä¸å‹•ç”¢ã€å» æˆ¿åŠè¨­å‚™': 'B02700', 
    'å–å¾—ä¸å‹•ç”¢åŠè¨­å‚™': 'B02700'
    # 'è³¼ç½®å›ºå®šè³‡ç”¢': 'B02700', # before 2013
    # 'è³¼ç½®è³‡ç”¢': 'B02700' # before 2013
}]


def parse_func(param, connection):
    ''' 
        Parse data of a season and store it into sqlite table.

        param (str, str, str): (stock id, year, season) 
        cursor: sqlite cursor
        
        Note: Data dependency: Q4 -> Q3 -> Q2 -> Q1

        Note: If you call this function with the same parameters again, the original data will be replaced by new data.

        Note: company id should NOT be provided with ".TW" suffix

        Note: æç›Šè¡¨
        1. for Q4, we will get the accumulated data, so we should subtract it from Q1 + Q2 + Q3 data

        Note: ç¾é‡‘æµé‡è¡¨
        1. for Q2, Q3 & Q4, we will get the accumulated data, so we should subtract it from the data of all previous seasons
    '''

    cursor = connection.cursor()
    table = table_name(param)
    id, Year, Quarter = param

    # ####### dependency check #######
    # table_list = set()
    # ls_table_query = 'SELECT name FROM sqlite_schema WHERE type ="table" AND name NOT LIKE "sqlite_%";'
    # for row in cursor.execute(ls_table_query):  table_list.add(row[0])

    # # dependency: all previous seasons in the same year
    # # ex: Q4 dependency: Q1, Q2, Q3
    # # check order: Q1 -> Q2 -> Q3
    # if Quarter in ['2', '3', '4']:
    #     L = int(Quarter)
    #     for season in range(1, L):
    #         new_param = id, Year, str(season)
    #         if table_name(new_param) not in table_list:
    #             print(f'\nMissing required dependency: Q{season}')
    #             print(f'Start parsing dependencies...')
    #             parse_func(new_param, connection)
        
    print(f'[INFO] ğŸŸ¢ Start parser')

    try:
        # year 2019 ~
        if int(Year) >= 2019:
            dfs = _parse_func_helper(param, cursor)
        # year 2013 ~ 2018
        elif int(Year) >= 2013:
            dfs = _parse_func_helper_2013_2018(param, cursor)
        else:
            dfs = _parse_func_helper_before_2013(param, cursor)

        ####### extra processing #######
        # process Q4 æç›Šè¡¨, 2013ä¹‹å‰çš„æç›Šè¡¨ç‚ºç´¯è¨ˆè³‡æ–™
        if Quarter == '4' or (int(Year) < 2013 and Quarter in ['2', '3', '4']):
            income_Code = dfs[1]['Code'].to_list()
            _process_accumulated_data(param, cursor, income_Code)

        # process Q2 or Q3 or Q4 ç¾é‡‘æµé‡è¡¨
        if Quarter in ['2', '3', '4']:
            cash_Code = dfs[2]['Code'].to_list()
            _process_accumulated_data(param, cursor, cash_Code)
    except Exception as e:
        error_message = f'[ERROR] âŒ parsing: {table}\n'
        logger.write(error_message)
        print(error_message)
        traceback.print_exc()
        # connection.rollback()   # not needed
        return

    
    connection.commit()
    print(f'\n[INFO] âœ… Complete parsing: {table}')
    print('----------------------------------------------------------')


def _parse_func_helper(param, cursor):
    ''' 
        parsing core function (2019 ~)

        1. insert parsed data (sqlite)
        2. return pandas table [è³‡ç”¢è² å‚µè¡¨, æç›Šè¡¨, ç¾é‡‘æµé‡è¡¨]
    '''
    table = table_name(param)
    id, Year, Quarter = param
    url = f'https://mops.twse.com.tw/server-java/t164sb01?step=1&CO_ID={id}&SYEAR={Year}&SSEASON={Quarter}&REPORT_ID=C'


    # create current table (sqlite)
    query = 'CREATE TABLE IF NOT EXISTS ' + table + ' (\
                Code TEXT, \
                Title TEXT, \
                Money BIGINT \
                );'
    print('[INFO] query: ' + query) 
    cursor.execute(query)

    # clear data (sqlite)
    query = f'DELETE FROM {table};'
    print('[INFO] query: ' + query)
    cursor.execute(query)

    # parsing
    print(f'[INFO] parsing: {url}\n')
    res = requests.get(url)
    res.encoding = 'big5'

    try:
        dfs = pd.read_html(StringIO(res.text))[0:3]
    except ValueError:
        print('[ERROR] data not available..., could be due to too many request')
        raise RequestError

    try:
        # must have three sheets
        for i in range(3):
            # must have three columns
            if len(dfs[i].columns) < 3:
                raise ValueError
    except (ValueError, IndexError):
        print('[ERROR] data not available..., could be due to too many request')
        raise RequestError

    # dfs[0]    è³‡ç”¢è² å‚µè¡¨
    # dfs[1]    æç›Šè¡¨
    # dfs[2]    ç¾é‡‘æµé‡è¡¨

    # clear old csv
    path = os.path.join('..', 'csv', 'financial_info', f"{table}.csv")
    if os.path.exists(path):
        os.remove(path)

    # save three financial statements
    for i in range(3):
        # drop useless columns
        dfs[i] = dfs[i].iloc[:, 0:3]
        dfs[i].columns = ['Code', 'Title', 'Money']

        # drop useless rows (that has blank in "Code")
        dfs[i].dropna(subset='Code', inplace = True)

        # all columns into string (for ease of regex processing)
        dfs[i] = dfs[i].astype('string')

        # eliminate ".0" induced by float type
        dfs[i]['Code'] = dfs[i]['Code'].str.replace('\.0','', regex=True)
        
        # eliminate right parenthesis and commas, replace left parenthesis with minus sign (negative number)
        pat_repl_dict = {'(': '-', ')':'', ',':''}
        for pat, repl in pat_repl_dict.items():
            dfs[i]['Money'] = dfs[i]['Money'].str.replace(pat, repl, regex=True)
        
        # convert "Money" to numeric
        dfs[i]['Money'] = pd.to_numeric(dfs[i]['Money'])

        # convert to standardized Codeï¼ˆfinancial companies é‡‘èå…¬å¸)
        for key, val in Standard_Code_mapping[i].items():
            dfs[i].loc[dfs[i]['Code'] == key , 'Code'] = val
        
        # load data into sqlite
        data = dfs[i].values.tolist()
        query = 'INSERT INTO ' + table + ' VALUES (?, ?, ?);'
        cursor.executemany(query, data)

        # save csv (additional records)
        dfs[i].to_csv(path, encoding='utf-8', index=False, mode="a")


    # clear duplicate rows
    # ref: https://dba.stackexchange.com/questions/116868/sqlite3-remove-duplicates
    cursor.execute(f'DELETE FROM {table} WHERE rowid NOT IN (SELECT min(rowid) FROM {table} GROUP BY Code)')

    return dfs

def _parse_func_helper_2013_2018(param, cursor):
    ''' 
        parsing core function (2013 ~ 2018)

        1. insert parsed data (sqlite)
        2. return pandas table, [è³‡ç”¢è² å‚µè¡¨, æç›Šè¡¨, ç¾é‡‘æµé‡è¡¨]
    '''
    table = table_name(param)
    id, Year, Quarter = param
    url = f'https://mops.twse.com.tw/server-java/t164sb01?step=1&CO_ID={id}&SYEAR={Year}&SSEASON={Quarter}&REPORT_ID=C'

    # create current table (sqlite)
    query = 'CREATE TABLE IF NOT EXISTS ' + table + ' (\
                Code TEXT, \
                Money BIGINT \
                );'
    print('[INFO] query: ' + query) 
    cursor.execute(query)

    # clear data (sqlite)
    query = f'DELETE FROM {table};'
    print('[INFO] query: ' + query)
    cursor.execute(query)

    # parsing
    print(f'[INFO] parsing: {url}\n')
    res = requests.get(url)
    res.encoding = 'big5'

    try:
        dfs = pd.read_html(StringIO(res.text))[1:4]
    except ValueError:
        print('[ERROR] data not available..., could be due to too many request')
        raise RequestError

    try:
        # must have three sheets
        for i in range(3):
            # must have three columns
            if len(dfs[i].columns) < 2:
                raise ValueError
    except (ValueError, IndexError):
        print('[ERROR] data not available..., could be due to too many request')
        raise RequestError
    
    # dfs[1]    è³‡ç”¢è² å‚µè¡¨
    # dfs[2]    æç›Šè¡¨
    # dfs[3]    ç¾é‡‘æµé‡è¡¨

    # clear old csv
    path = os.path.join('..', 'csv', 'financial_info', f"{table}.csv")
    if os.path.exists(path):
        os.remove(path)

    # save three financial statements
    for i in range(3):
        # drop useless columns
        dfs[i] = dfs[i].iloc[:, 0:2]
        dfs[i].columns = ['Code', 'Money']

        # drop useless rows (that has blank in "Money")
        dfs[i].dropna(subset='Money', inplace = True)

        # change Chinese Title to Code
        for key, val in Standard_Code_mapping[i].items():
            dfs[i].loc[dfs[i]['Code'] == key , 'Code'] = val

        # load data into sqlite
        data = dfs[i].values.tolist()
        query = 'INSERT INTO ' + table + ' VALUES (?, ?);'
        cursor.executemany(query, data)

        # save csv (additional records)
        dfs[i].to_csv(path, encoding='utf-8', index=False, mode="a")

    # clear duplicate rows
    # ref: https://dba.stackexchange.com/questions/116868/sqlite3-remove-duplicates
    cursor.execute(f'DELETE FROM {table} WHERE rowid NOT IN (SELECT min(rowid) FROM {table} GROUP BY Code)')

    return dfs


def _parse_func_helper_before_2013(param, cursor):
    ''' 
        (deprecated, due to data inconsistency)
        parsing core function (~2012)

        1. insert parsed data (sqlite)
        2. return pandas table, [è³‡ç”¢è² å‚µè¡¨, æç›Šè¡¨, ç¾é‡‘æµé‡è¡¨]
    '''
    raise Exception("ğŸ”´ Deprecated functions")
    table = table_name(param)
    id, Year, Quarter = param
    Year = str(int(Year) - 1911)    # è¥¿å…ƒ -> æ°‘åœ‹
    sector = INPUT_DF[INPUT_DF['CO_ID'] == id+'.TW']['Sector'].values[0]

    url = [ 'https://mops.twse.com.tw/mops/web/ajax_t05st33',  # è³‡ç”¢è¡¨
            'https://mops.twse.com.tw/mops/web/ajax_t05st34',  # æç›Šè¡¨
            'https://mops.twse.com.tw/mops/web/ajax_t05st39'  # ç¾é‡‘è¡¨
    ]

    # create current table (sqlite)
    query = 'CREATE TABLE IF NOT EXISTS ' + table + ' (\
                Code TEXT, \
                Money BIGINT \
                );'
    print('[QUERY] ' + query) 
    cursor.execute(query)

    # clear data (sqlite)
    query = f'DELETE FROM {table};'
    print('[QUERY] ' + query)
    cursor.execute(query)

    # check if this is a financial company
    is_fin_co = (sector == 'Financial Services')

    final_dfs = []

    # save three financial statements
    for i in range(3):
        payload = {
            'co_id': id,
            'year': Year,
            'season': '0' + Quarter,
            'firstin': '1',
            'check2858': 'Y',
            # 'checkbtn': '',
            # 'step': '1',
            # 'queryName': 'co_id',
            # 'TYPEK2': '',
            # 'inpuType': 'co_id',
            # 'off': '1',
            # 'year': 'Y',
            # 'keyword4': '',
            # 'isnew': 'false',
            # 'code1': '',
            # 'encodeURIComponent': '1',
            # 'TYPEK': 'sii'
        }

        try:
            print(f'parsing: {url[i]}\n')
            res = requests.post(url[i], data=payload)
            dfs = pd.read_html(StringIO(res.text))
        except ValueError:
            print('[Error]: data not available..., could be due to too many request')
            raise RequestError
        
        if i != 2:  # è³‡ç”¢è¡¨, æç›Šè¡¨
            # table is located at index 2
            # drop useless rows, columns
            df = dfs[-1].iloc[4:, 0:2]
            
            df.columns = ['Code', 'Money']

            # drop useless rows (that has blank in "Money")
            df.dropna(subset='Money', inplace = True)

            # remove space
            df['Code'] = df['Code'].str.replace(' ','', regex=True)

            df['Money'] = pd.to_numeric(df['Money'])

            # change Chinese Title to Standard Code
            for key, val in Standard_Code_mapping[i].items():
                df.loc[df['Code'] == key , 'Code'] = val
            
            if i == 0:  # only è³‡ç”¢è¡¨
                if not is_fin_co: # ä¸€èˆ¬å…¬å¸ï¼ŒNOT é‡‘èå…¬å¸
                    query_money = lambda key: df.loc[df['Code'] == key , 'Money'].values[0]
                    # éæµå‹•è³‡ç”¢åˆè¨ˆ: 15XX
                    amount = query_money('1XXX') - query_money('11XX')
                    row = pd.DataFrame({'Code':['15XX'], 'Money': [amount]})
                    df = pd.concat([df, row], ignore_index=True)

                    # éæµå‹•è² å‚µåˆè¨ˆ: 25XX
                    amount = query_money('è² å‚µç¸½è¨ˆ') - query_money('21XX')
                    row = pd.DataFrame({'Code':['25XX'], 'Money': [amount]})
                    df = pd.concat([df, row], ignore_index=True)

                # è‚¡æœ¬: 3100
                if len(df[df['Code'] == 'è‚¡æœ¬']) == 0:
                    # è‚¡æœ¬ = æ™®é€šè‚¡è‚¡æœ¬
                    df.loc[df['Code'] == 'æ™®é€šè‚¡è‚¡æœ¬', 'Code'] = '3100'

        else:   # ç¾é‡‘è¡¨
            # Note: ç¾é‡‘è¡¨æ˜¯ raw text, not table, å› æ­¤åˆ©ç”¨string splitï¼Œå†linear scan

            tmp = dfs[-1].values[0][0].replace('\u3000', '')    # remove Ideographic space
            tmp = tmp.split(' ')

            tmp_dict = {'Code':[], 'Money': []}

            for i in range(len(tmp)):
                # find target values
                if tmp[i] in Standard_Code_mapping[2]:
                    code = Standard_Code_mapping[2][tmp[i]]
                    
                    if len(tmp[i+1]) == 1:  # handle bad formatted data
                        value = tmp[i+2]
                    else:
                        value = tmp[i+1]

                    if '(' in value or ')' in value:
                        value = '-' + value # negative

                    value = value.replace(',', '')
                    value = value.replace('(', '')
                    value = value.replace(')', '')
                    value = int(value) # to numeric value

                    tmp_dict['Code'].append(code)
                    tmp_dict['Money'].append(value)
                    
                    if code == 'B02700':   # handle duplicates
                        break
            df = pd.DataFrame(tmp_dict) # build df

        final_dfs.append(df)

        # load data into sqlite
        data = df.values.tolist()
        query = 'INSERT INTO ' + table + ' VALUES (?, ?);'
        cursor.executemany(query, data)

        # dfs[i].to_csv(path, encoding='utf-8', index=False)

    # clear duplicate rows
    # ref: https://dba.stackexchange.com/questions/116868/sqlite3-remove-duplicates
    cursor.execute(f'DELETE FROM {table} WHERE rowid NOT IN (SELECT min(rowid) FROM {table} GROUP BY Code)')

    return final_dfs

def _process_accumulated_data(param, cursor, Codes):
    '''
        Given codes that have accumulated data, calculate their non-accumulated data
        ex: Q4 - (Q1+Q2+Q3)
            Q3 - (Q1+Q2)
            Q2 - (Q1)

        Note: If there is no matching code (possibly missing) in the previous quarters, assume its value zero. 
            Usually, target codes will have their counterparts in the previous quarters.
    '''
    print("[INFO] Process accumulated data: ")
    print(Codes)

    table = table_name(param)
    id, Year, Quarter = param
    
    L = int(Quarter)

    # accumulated value -> non-accumulated value
    for code in Codes:
        cur_query = f'SELECT Money FROM {table} WHERE Code = "{code}"'
        cur_val = cursor.execute(cur_query).fetchone()[0]

        for season in range(1, L):
            prev_param = [id, Year, str(season)]
            prev_table = table_name(prev_param)

            prev_query = f'SELECT Money FROM {prev_table} WHERE Code = "{code}"'
            prev_val = cursor.execute(prev_query).fetchone()

            # note: sometimes, previous value does not exist
            if prev_val is not None:
                cur_val -= prev_val[0]
    
        # update values in db
        query = f'UPDATE {table} SET Money={cur_val} WHERE Code = "{code}"'
        cursor.execute(query)
